{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06faf319",
   "metadata": {},
   "source": [
    "# Parte - 1 Introdução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386f007b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "Collecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853646 sha256=f732e2a652949f286af29a883b5ac820396278aecf97c2fa936bad7f779c68c0\n",
      "  Stored in directory: c:\\users\\feoxp7\\appdata\\local\\pip\\cache\\wheels\\52\\45\\50\\69db7b6e1da74a1b9fcc097827db9185cb8627117de852731e\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\feoxp7\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\feoxp7\\appdata\\local\\pip\\cache\\wheels\\52\\45\\50\\69db7b6e1da74a1b9fcc097827db9185cb8627117de852731e\\pyspark-3.2.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: py4j==0.10.9.3 in c:\\users\\feoxp7\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyspark) (0.10.9.3)\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\feoxp7\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5f160be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7df46e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Leonardo</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Leticia</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marisa</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paulo</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name  Age\n",
       "0  Leonardo   29\n",
       "1   Leticia   26\n",
       "2    Marisa   54\n",
       "3     Paulo   56"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d8caabc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://PL1BRSSU0250NB.net.plm.eds.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practica</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2ae910b8af0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Practica').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "194160be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|     _c0|_c1|       _c2|\n",
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "|Leonardo| 29|         5|\n",
      "|   Paulo| 56|         5|\n",
      "|  Marisa| 54|         5|\n",
      "| Leticia| 26|         5|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('data.csv', sep=';')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6eecf673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|Leonardo| 29|         5|\n",
      "|   Paulo| 56|         5|\n",
      "|  Marisa| 54|         5|\n",
      "| Leticia| 26|         5|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header', 'true').csv('data.csv', sep=';').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cea28908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcff4f2",
   "metadata": {},
   "source": [
    "# Parte 2 - DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc07f9",
   "metadata": {},
   "source": [
    "* Pyspark Dataframe\n",
    "* Lendo o dataset\n",
    "* Verificando datatypes das colunas (Schema)\n",
    "* Selecionando Colunas e indexação\n",
    "* Check Describe option similar to Pandas\n",
    "* Adicionando Colunas\n",
    "* Dropando Colunas\n",
    "* Renomeando Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cde42e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://PL1BRSSU0250NB.net.plm.eds.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practica</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2ae910b8af0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Practica - DataFrame').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c9edf987",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lendo o dataset \n",
    "df_spark = spark.read.option('header', 'true').csv('data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "573d1063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "50a9982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veja que todas as informações foram tratadas como strings. Até mesmo a idade. \n",
    "#Para corrigir isso, usamos o parametro inferSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6e527b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lendo o dataset \n",
    "df_spark = spark.read.option('header', 'true').csv('data.csv', sep=';', inferSchema=True)\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "641f0a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|Leonardo| 29|         5|\n",
      "|   Paulo| 56|         5|\n",
      "|  Marisa| 54|         5|\n",
      "| Leticia| 26|         5|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.csv('data.csv', sep=';', header=True, inferSchema=True)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b5ae5d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Verificando o schema\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5f67c6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n",
      "+---+--------+\n",
      "|Age|    Name|\n",
      "+---+--------+\n",
      "| 29|Leonardo|\n",
      "| 56|   Paulo|\n",
      "| 54|  Marisa|\n",
      "| 26| Leticia|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Selecting Columns \n",
    "df_spark.select('Age', 'Name').printSchema()\n",
    "df_spark.select('Age', 'Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d2706189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "29b4c523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Name: string, Age: string, Experience: string]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "751e3037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+----------+\n",
      "|summary|    Name|               Age|Experience|\n",
      "+-------+--------+------------------+----------+\n",
      "|  count|       4|                 4|         4|\n",
      "|   mean|    null|             41.25|       5.0|\n",
      "| stddev|    null|15.945218719101975|       0.0|\n",
      "|    min|Leonardo|                26|         5|\n",
      "|    max|   Paulo|                56|         5|\n",
      "+-------+--------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Algumas informações importantes\n",
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7d06ec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+-----------------------+\n",
      "|    Name|Age|Experience|Experience After 2 Year|\n",
      "+--------+---+----------+-----------------------+\n",
      "|Leonardo| 29|         5|                      7|\n",
      "|   Paulo| 56|         5|                      7|\n",
      "|  Marisa| 54|         5|                      7|\n",
      "| Leticia| 26|         5|                      7|\n",
      "+--------+---+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adicionando colunas em um dataframe \n",
    "df_spark.withColumn('Experience After 2 Year', df_spark['Experience']+2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4640379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|Leonardo| 29|         5|\n",
      "|   Paulo| 56|         5|\n",
      "|  Marisa| 54|         5|\n",
      "| Leticia| 26|         5|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Removendo colunas\n",
    "df_spark = df_spark.drop('Experience After 2 Year')\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f11a2ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|New Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|Leonardo| 29|         5|\n",
      "|   Paulo| 56|         5|\n",
      "|  Marisa| 54|         5|\n",
      "| Leticia| 26|         5|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renomeando colunas \n",
    "df_spark.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d625199d",
   "metadata": {},
   "source": [
    "#  Parte 3 - Lidando com valores faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b85a5",
   "metadata": {},
   "source": [
    "* Dropando Colunas\n",
    "* Dropando Linhas \n",
    "* Vários parâmetros em drop funcionalidades\n",
    "* Lidando com valores ausentes com média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3892d72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://PL1BRSSU0250NB.net.plm.eds.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practica</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2ae910b8af0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Practica - DataFrame').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "02d58de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+\n",
      "|    Name| Age|Experience|\n",
      "+--------+----+----------+\n",
      "|Leonardo|  29|         5|\n",
      "|   Paulo|  56|         5|\n",
      "|  Marisa|  54|         5|\n",
      "| Leticia|  26|         5|\n",
      "|   Heron|null|         4|\n",
      "| Pitucha|  25|      null|\n",
      "|    null|  35|      null|\n",
      "+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lendo o dataset \n",
    "df_spark = spark.read.option('header', 'true').csv('data2.csv', sep=';', inferSchema=True)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b013513f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|    Name| Age|\n",
      "+--------+----+\n",
      "|Leonardo|  29|\n",
      "|   Paulo|  56|\n",
      "|  Marisa|  54|\n",
      "| Leticia|  26|\n",
      "|   Heron|null|\n",
      "| Pitucha|  25|\n",
      "|    null|  35|\n",
      "+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop a coluna\n",
    "df_spark.drop('Experience').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f8bdbfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|Leonardo| 29|         5|\n",
      "|   Paulo| 56|         5|\n",
      "|  Marisa| 54|         5|\n",
      "| Leticia| 26|         5|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6da37e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+\n",
      "|    Name| Age|Experience|\n",
      "+--------+----+----------+\n",
      "|Leonardo|  29|         5|\n",
      "|   Paulo|  56|         5|\n",
      "|  Marisa|  54|         5|\n",
      "| Leticia|  26|         5|\n",
      "|   Heron|null|         4|\n",
      "| Pitucha|  25|      null|\n",
      "|    null|  35|      null|\n",
      "+--------+----+----------+\n",
      "\n",
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|Leonardo| 29|         5|\n",
      "|   Paulo| 56|         5|\n",
      "|  Marisa| 54|         5|\n",
      "| Leticia| 26|         5|\n",
      "+--------+---+----------+\n",
      "\n",
      "+--------+----+----------+\n",
      "|    Name| Age|Experience|\n",
      "+--------+----+----------+\n",
      "|Leonardo|  29|         5|\n",
      "|   Paulo|  56|         5|\n",
      "|  Marisa|  54|         5|\n",
      "| Leticia|  26|         5|\n",
      "|   Heron|null|         4|\n",
      "| Pitucha|  25|      null|\n",
      "|    null|  35|      null|\n",
      "+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop linhas que tenho null values em todos os campos\n",
    "df_spark.na.drop(how=\"all\").show()\n",
    "\n",
    "#Drop linhas que tenho null values em pelo menos um campo\n",
    "df_spark.na.drop(how=\"any\").show()\n",
    "\n",
    "#Drop linhas que tenho null values em + de 1 campo (o 1 é definido pelo parametro thresh)\n",
    "df_spark.na.drop(how=\"any\", thresh=1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "be1041b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+\n",
      "|    Name| Age|Experience|\n",
      "+--------+----+----------+\n",
      "|Leonardo|  29|         5|\n",
      "|   Paulo|  56|         5|\n",
      "|  Marisa|  54|         5|\n",
      "| Leticia|  26|         5|\n",
      "|   Heron|null|         4|\n",
      "+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SubSet - Dropa linhas se algum valor de uma coluna especifica estiver como null\n",
    "df_spark.na.drop(how=\"any\", subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a96d07cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+\n",
      "|         Name| Age|Experience|\n",
      "+-------------+----+----------+\n",
      "|     Leonardo|  29|         5|\n",
      "|        Paulo|  56|         5|\n",
      "|       Marisa|  54|         5|\n",
      "|      Leticia|  26|         5|\n",
      "|        Heron|null|         4|\n",
      "|      Pitucha|  25|      null|\n",
      "|Missing Value|  35|      null|\n",
      "+-------------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filling the Missing Value\n",
    "df_spark.na.fill('Missing Value').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c223e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preenchendo valores com algumas estratégias\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Age', 'Experience'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['Age', 'Experience']]\n",
    ").setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "64c48b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+-----------+------------------+\n",
      "|    Name| Age|Experience|Age_imputed|Experience_imputed|\n",
      "+--------+----+----------+-----------+------------------+\n",
      "|Leonardo|  29|         5|         29|                 5|\n",
      "|   Paulo|  56|         5|         56|                 5|\n",
      "|  Marisa|  54|         5|         54|                 5|\n",
      "| Leticia|  26|         5|         26|                 5|\n",
      "|   Heron|null|         4|         37|                 4|\n",
      "| Pitucha|  25|      null|         25|                 4|\n",
      "|    null|  35|      null|         35|                 4|\n",
      "+--------+----+----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Add imputation cols to df \n",
    "imputer.fit(df_spark).transform(df_spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f730a",
   "metadata": {},
   "source": [
    "#  Parte 4 - Operações com Filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbbafe9",
   "metadata": {},
   "source": [
    "* &, |, ==\n",
    "* ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9f64cd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://PL1BRSSU0250NB.net.plm.eds.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practica</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2ae910b8af0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Practica - DataFrame').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ea2dd943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lendo o dataset \n",
    "df_spark = spark.read.option('header', 'true').csv('data3.csv', sep=';', inferSchema=True)\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c0d761fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "| Marisa| 54|         5|  5200|\n",
      "|Leticia| 26|         5| 15000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Salario das pessoas igual ou maior que 5000\n",
    "df_spark.filter(\"Salary>5000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7877f8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|Age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "|Marisa| 54|         5|  5200|\n",
      "+------+---+----------+------+\n",
      "\n",
      "+--------+---+----------+------+\n",
      "|    Name|Age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|Leonardo| 29|         5|  4000|\n",
      "|   Paulo| 56|         5|  5000|\n",
      "|  Marisa| 54|         5|  5200|\n",
      "| Leticia| 26|         5| 15000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.filter((df_spark['Salary']>5000) & (df_spark['Salary']<15000)).show()\n",
    "df_spark.filter((df_spark['Salary']>5000) | (df_spark['Salary']<15000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eaf80a",
   "metadata": {},
   "source": [
    "#  Parte 5 - Group By e funções de agregação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b60a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practica - DataFrame').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "862c43e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departament: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lendo o dataset \n",
    "df = spark.read.option('header', 'true').csv('data4.csv', sep=';', inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "da228190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|    Name| Departament|Salary|\n",
      "+--------+------------+------+\n",
      "|Leonardo|Data Science| 10000|\n",
      "|   Paulo|         IOT|  5000|\n",
      "|  Marisa|    Big Data|  4000|\n",
      "| Leticia|    Big Data|  4000|\n",
      "|   Bruno|Data Science|  3000|\n",
      "|   Pedro|Data Science| 10000|\n",
      "|Caroline|         IOT|  5000|\n",
      "+--------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "835303c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|    Name|sum(Salary)|\n",
      "+--------+-----------+\n",
      "|   Paulo|       5000|\n",
      "|   Bruno|       3000|\n",
      "|Leonardo|      15000|\n",
      "|   Pedro|      10000|\n",
      "|  Marisa|       4000|\n",
      "|Caroline|       5000|\n",
      "| Leticia|       4000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GroupBy\n",
    "#Group para achar a soma dos salários por pessoa\n",
    "df.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d0353334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departament|sum(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|       8000|\n",
      "|Data Science|      23000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GroupBy\n",
    "#Group para achar a soma dos salários por departamento\n",
    "df.groupBy('Departament').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "df3eed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departament|count|\n",
      "+------------+-----+\n",
      "|         IOT|    3|\n",
      "|    Big Data|    2|\n",
      "|Data Science|    3|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GroupBy\n",
    "#Group para achar número de funcionários por departamento\n",
    "df.groupBy('Departament').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ef1d4d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      46000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " df.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4218b2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|    Name|max(Salary)|\n",
      "+--------+-----------+\n",
      "|   Paulo|       5000|\n",
      "|   Bruno|       3000|\n",
      "|Leonardo|      10000|\n",
      "|   Pedro|      10000|\n",
      "|  Marisa|       4000|\n",
      "|Caroline|       5000|\n",
      "| Leticia|       4000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Name').max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130acb2",
   "metadata": {},
   "source": [
    "# Parte 6 - Machine Learning com DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d0f19fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nessa parte vamos usar um modelo simples de machine learning para tentar prever um salário de um funcionário\n",
    "# com base na sua idade e anos de experiência. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5f492be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://PL1BRSSU0250NB.net.plm.eds.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practica</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2ae910b8af0>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Practica - DataFrame').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "24e4f2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lendo o dataset \n",
    "df = spark.read.option('header', 'true').csv('data3.csv', sep=';', inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a1bfe279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|Age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|Leonardo| 29|         5|  4000|\n",
      "|   Paulo| 56|         5|  5000|\n",
      "|  Marisa| 54|         5|  5200|\n",
      "| Leticia| 26|         5| 15000|\n",
      "|   Maria| 84|        10| 20000|\n",
      "|   Heron|  5|         4|  5000|\n",
      "| Pitucha|  5|         4|  5000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "44074c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+--------------------+\n",
      "|    Name|Age|Experience|Salary|Independent Features|\n",
      "+--------+---+----------+------+--------------------+\n",
      "|Leonardo| 29|         5|  4000|          [29.0,5.0]|\n",
      "|   Paulo| 30|         5|  5000|          [30.0,5.0]|\n",
      "|  Marisa| 35|         5|  5200|          [35.0,5.0]|\n",
      "| Leticia| 26|         5| 15000|          [26.0,5.0]|\n",
      "|   Maria| 40|        10| 20000|         [40.0,10.0]|\n",
      "|   Heron| 15|         4|  5000|          [15.0,4.0]|\n",
      "| Pitucha| 20|         4|  5000|          [20.0,4.0]|\n",
      "+--------+---+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler = VectorAssembler(inputCols=[\"Age\", \"Experience\"], outputCol=\"Independent Features\")\n",
    "output = featureassembler.transform(df)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "854cc0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent Features|Salary|\n",
      "+--------------------+------+\n",
      "|          [29.0,5.0]|  4000|\n",
      "|          [30.0,5.0]|  5000|\n",
      "|          [35.0,5.0]|  5200|\n",
      "|          [26.0,5.0]| 15000|\n",
      "|         [40.0,10.0]| 20000|\n",
      "|          [15.0,4.0]|  5000|\n",
      "|          [20.0,4.0]|  5000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data = output.select(\"Independent Features\", \"Salary\")\n",
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "449ff46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression \n",
    "\n",
    "#Treina com 75% e testa com 25%\n",
    "train_data, test_data = finalized_data.randomSplit([0.75, 0.25])\n",
    "regressor = LinearRegression(featuresCol='Independent Features', labelCol='Salary')\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b7408b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-110.1847, 2740.8495])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coeficientes\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "39f59853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2311.7321964314488"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interceptações\n",
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fc2bb87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------------+\n",
      "|Independent Features|Salary|       prediction|\n",
      "+--------------------+------+-----------------+\n",
      "|          [29.0,5.0]|  4000|8197.157745144495|\n",
      "|          [30.0,5.0]|  5000|8086.972998578893|\n",
      "+--------------------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predição \n",
    "pred = regressor.evaluate(test_data)\n",
    "pred.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96697ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
